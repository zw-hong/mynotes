本周学的东西：

- 学了点pytoch（动手学深度学习）
- 机器翻译基础与模型两章
- paper reading



## Sequence to Sequence Learning with Nerual Networks (NIPS 2014)

#### Abstract:

encoder: Multilayered LSTM 

Decoder: deep LSTM

主要结果：

- WMT-14 En-Fr BLEU：34.8，phrase-based SMT仅为33.3
- LSTM对长句子也可以
- LSTM去rerank由SMT 产生的1000个句子的时候，BLEU为36.5
- LSTM还学习到了phrase and sentence representations 
- 在源端对单词顺序进行改变，可以显著的提高LSTM的性能**because doing so introduced many short term dependencies between the source and the target sentence which made the optimization easier**

#### conclusion:

- LSTM with a limited vocabulary > a standard SMT-based system with an unlimited scale task
- by reversing the words in the source sentences可以使得性能提升，并且模型容易训练。（作者认为RNN也应该可以，但是没做实验）**short term dependecies** 
-  可以很好的翻译长句子（可能是因为reverse的缘故）
- 最重要的是：sequence to sequence 可以用一个simple, straightforward and unoptimized 方法解决

#### Related work

- by rescoring the n-best lists of a strong MT baseline 
- 在NNLM中加入源端句子的信息
    - topic
    - use the decoder‘s alignment information  to provide the NNLM with the most useful words in the input sentence 
- encoder-decoder的机制
    - Kal等人，使用的是卷积，丢失了句子的顺序信息
    - 引入注意力机制，克服长句子性能不佳
    - 将长句子->切分为片段，进行翻译，解决内存问题
- End-to-end（并非直接产生翻译）



#### introduction：

DNNs在处理sequence to sequence 的难点是因为它只能处理源端和目标端都是固定长度的任务。

The idea is to use LSTM to read the input sequence, one timestep at a time, to obtain large fixed diemensional vector representation, and then to use another LSTM to extract the output sequence from the vector.采用LSTM的原因也是因为可以考虑到长句子中各个单词的considerable time lag 信息

![截屏2022-07-25 上午11.25.22](/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-07-25 上午11.25.22.png)

实验结果主要有：

- WMT14 En-Fr：BlEU34.81，direct translation，5层LSTM ensemble（SMT baseline 仅仅为33.30）
- 使用LSTM去rescore 1000-best lists of SMT baseline的时候，提高了3.2BLEU
- Reverse the words in the sentence 在本文的工作中是最创新的贡献之一，可以解决长句子翻译问题， 也可以解决优化梯度下降问题。
- LSTM可以将variable长度的input转变为fixed-dimensional representation。
- our model is aware of word order and is fairly invariant to the active and passive voice （但是对句子之间的顺序很敏感）

![截屏2022-07-25 上午11.42.16](/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-07-25 上午11.42.16.png)

#### The model

- 两个LSTM，好处是计算量不大，模型参数增加，并且可以同时train不同的语言对
- deep LSTM > shallow LSTM
- reverse the word order.我觉的应该是倒过来之后，句子前面的部分就很容易翻译，后面的部分又依赖于前面的部分，因此导致优化速度较快。



#### Experiments

训练目标函数
$$
\frac1{|\mathcal{S}|}\sum_{(T,S)\in|mathcal{S}}log \text{ p } (T|S)
$$
训练结束后，找到最佳的翻译句子:
$$
\hat T = \arg \max_{T}p(T|S)
$$


reversing the source sentences的作用只会对目标句子的前部分更加确定，后半部分不好说。





## TransQuest: Translation Quality Estimation with cross-lingual Transformers (COLING 2020)

source - reference - score 

#### Abstract：

neral-based architectures 可以极大的提高QE，但是大部分这些模型只能适用于训练集和测试集一样的语句对。

在transfer learning上非常有用，并且可以处理低资源语言对。

本文提出的模型：**cross-lingual transformers**

#### Conclusion：

- outperforms on both aspects of Sentence-level quality estimation (人为打分， 修改率)
-  it is the first time that a QE system explores multi-language pair models and
    transfer learning on low-resource language pairs.
- do not use parallel data 
- MTransQuest & STransQuest 

future to do:

- Word-level and document-level 
- sentence-level: perform transfer learning on language pairs that do not include English at all
- conduct unspervised experiments on low-resource language pairs 

#### Introduction：

QE: 评估没有reference的翻译

本文提出的是一种简单的模型结构：it can be easily trained with different types of inputs(i.e **different language pairs or language from different domains** ) and can be used for transfer learning in settings where there is not enough training data 

本文的主要贡献：

- 开源框架TransQuest，两种网络结构，outperform SOTA in two different aspects of sentence-level QE
- 该QE框架可以评估超过一种语言对，解决了多语言对QE环境问题
- 该框架可以使用迁移学习，来解决QE中的低资源语料问题
- 开源社区的贡献



#### Related Work

- linguistic & processing & feature engineering ： QUEst++等工具

- POSTECH: encoder-decoder RNN (predictor) , bidirectional RNN(estimator)，缺点是POSTECH requires extensive predictor pre-training, which means it depends on large parallel data and is computationally intensive 

- OpenKiwi：四种模型中最好的是stacked模型，同样也有上面的问题

- 引入crosslingual embeddings that are already fine-tuned to reflect peoperties between languages 这样做的好处是，减少了不必要的大量平行语料训练。
- mBERT & DistilBERT 并不能在crosslingual benchmarks 取得好的分数
- XLM-R > mBERT
- 通过预训练模型来对source和target进行一个representation



#### Methodology 

![截屏2022-07-19 下午8.19.30](/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-07-19 下午8.19.30.png)



**MonoTransQuest:**

- source 【SEP】 target

- 三种pooling策略
    - CLS输出：**效果最好**
    - mean输出
    - max输出
-  MSE损失函数

**SiameseTransQuest：**

- 这种结构的设计在相关文献中最好
- source和target分开输入
- 三种pooling策略
    - CLS输出
    - mean输出：**效果最好**
    - max输出
-  MSE损失函数

#### evaluation and discussion

评分指标：

two standard measures:

- Human-mediated Trannslation Edit Rate (HTER)
    - 机器翻译到reference需要修改的次数
    - the texts are from a variety of domains
- Direct Assessment (DA)
    - 人工直接打分（0～100）

evaluton metric : **Pearson 相关系数**

实验分为三个部分：

- supervised single language pair quality estimation
- supervised multi-language pair QE（主要是用来解决在多种语言下的QE问题）
    - 英语-所有，所有-英语，训练集直接拼接【**这种考虑direction的方法最好**】
    - 直接拼接
    - 上述两种方法的评估是在各个语料上进行的
- Transfer learning based QE

![截屏2022-07-25 下午4.40.47](/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-07-25 下午4.40.47.png)

## TransQuest at WMT2020: Sentence-Level Direct Assesment (EMNLP2020)

【有微调的方法看一下,sefl-ensemble & data augmentation 】

#### Abstract:

a simple QE framework based on corss-lingual transformers 

#### Conclusion:

- XLM-R & XLM-R-large ensemble can highly improve the performance
- data augementation can highly improve the performance 
- future to do:
    - Plan to experiment more with the data augementation settings(semantically similiar sentences)
    - handle named entities properly （本文模型在这方面有大量出错）
    - Document-level quality estimation 

#### Introduction:

QE有不同的级别：

- document level
- sentence level
- word level 

TransQuest uses **crosslingual embeddings** to remove the dependency on large parallel data 

Crosslingual embeddings: that are already fine-tuned to reflect properties between languages 【使用这个可以大大地降低网络结构的复杂程度】

#### Evaluation, Results and Discussion 

刷榜策略：

- TransQuest with Ensemble: XLM-R-base & XLM-R-Large ensemble，权重比0.8:0.2
- Data Augmentation：WMT中其它的数据集



