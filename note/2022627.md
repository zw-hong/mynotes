# 2022.6.20

本周学的东西：

- COVID-19 Cases Prediction (Regression) 达到strong baseline 
- 李宏毅week2课程听完
- 看了一些深度学习方面的书





## Paper reading："Deep Residual Learning for Image Recognition"

<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-06-26 下午7.22.44.png" alt="截屏2022-06-26 下午7.22.44" style="zoom: 50%;" />



**Q: Deeper nerual networks are more difficult to train ?**

**A:**  神经网络的计算过程依靠的是【前向传播】和【误差反向传播】。误差反向传播用于更新网络参数，但是由于误差反向传播主要依靠于链式求导法则，即就是某一层的参数调整是依赖于它后边所有层与误差的偏导数的，这就使得不同层的参数的训练速度不同。靠近输入层的网络层容易发生梯度消失/爆炸，可以简单理解为当各层的偏导数比较小时，反传至输入层时候梯度就会特别小(消失)，反之就会特别大(爆炸)，==梯度非常大或者非常小就会使得网络参数更新速度过大/过小==。



**Q： 第二个划线句（没读懂，不知道在说些什么）？**

**A：**可能就是读不懂。文章后面会说。就是提出了一个残差结构，每一层学习的是一个$\mathcal{H(x)}-x$的东西，和输入层有关。



---



<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-06-26 下午7.37.35.png" alt="截屏2022-06-26 下午7.37.35" style="zoom:33%;" />

**A:** 在这两个图当中，所要说明的是：【不使用残差】的网络其是训练不动的==（即optimization)）==的问题，不仅仅是过拟合问提（==模型的网络层数越深，所带来的灵活度越高，也即容易overfitting==）【<---有数学原理证明】





## INTRODUCTION

Into的撰写是对摘要的一个扩充部分，也是对自己工作的一个简单的概述



<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-06-26 下午7.48.32.png" alt="截屏2022-06-26 下午7.48.32" style="zoom:33%;" />



**Q：为什么网络层堆叠的越深，越容易导致梯度爆炸/消失？**

**A：**造成梯度爆炸/消失的原因，一方面来自激活函数，一方面来自于链式求导。比如sigmoid函数，导致神经元的输出被限制在0到1之间，sigmoid的导数取值范围在0到0.25之间，若【初始化的网络权重小于1】，当层数变多时，就会出现梯度消失的问题，反之，当【权重很大时】，就会出现梯度爆炸的问题。

Solution : 换用ReLU，LeakyReLU，ELU等激活函数

ReLU：使得激活函数的导数恒定为1

LeakyReLU：包含ReLu所有优点，同时解决ReLU中区间为0带来的影响

ELU：也是为了解决ReLU区间为0的影响，【计算耗时】



**Q：normalized initialization?**

**A：**初始化的时候做的好一点，就是权重在随机初始化的时候，权重不要特别大 也不要 特别小



**Q: intermediate normalization layers ?**

**A:** BN方法（batch normalization），通过归一化，确保每个神经元的输出以规范的方式输出，从而解决梯度消失和梯度爆炸的问题

---



<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-06-26 下午8.03.33.png" alt="截屏2022-06-26 下午8.03.33" style="zoom:33%;" />



就是说深层的网络模型可以train的起来，但是随着不断的train，Loss却在上升，正如开头部分的两幅图片。==degradation==

按理说加入了identity layer的深层模型其training error不会高于其shallower的网络，但是==Optimizer优化器做不到== ，

Solution : residual learning framework，即如下图所示

<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-06-26 下午9.20.02.png" alt="截屏2022-06-26 下午9.20.02" style="zoom:33%;" />



## Deep Residual Learning 

<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-06-26 下午9.32.01.png" alt="截屏2022-06-26 下午9.32.01" style="zoom:33%;" />

**Q：projection shortcut ?**

**A：**在卷积层上做投影，其实就是用一个1 ✖️ 1的卷积层，空间维度上不做任何东西，在通道维度上做改变，使得输出通道是输入通道的2倍，这样就能使得残差连接的输入和输出对上了。使用步幅为2的原因是因为，当你的通道维度升了2倍后，宽和高通常都会被减半。



<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-06-27 上午11.36.09.png" alt="截屏2022-06-27 上午11.36.09" style="zoom:33%;" />

**Q：identity mapping ？ zero mapping ？ 什么是find the perturbations ??**

**A：**我的理解是，在残差网络中，原始的输入会和输出作为第二次的输入，这就是identity mapping， 如果是在原始的网络，输入直接是前一层的输出，就是zero mapping





## Experiments

<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-06-26 下午9.57.31.png" alt="截屏2022-06-26 下午9.57.31" style="zoom:33%;" />

<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-06-27 上午11.52.42.png" alt="截屏2022-06-27 上午11.52.42" style="zoom:33%;" />

**Q: 采用deeper Bottleneck architectures的原因？**

**A：**是因为作者用实验对比了Identity & projection shortcuts。说明了projection shortcuts其实是非必需的，引入虽然有性能的提升，但是计算复杂度上升了，训练昂贵。因此就直接使用了identity mapping。但是使用identity mapping就会出现一个问题：==维度不一致==，所以使用了bottleneck的技术，简单来说，就是一个block中三个layer：原始数据-降维-升维输出

<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-06-26 下午9.57.45.png" alt="截屏2022-06-26 下午9.57.45" style="zoom:33%;" />

----



<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-06-26 下午9.47.45.png" alt="截屏2022-06-26 下午9.45.48" style="zoom: 25%;" />

猛然下降的原因是因为，本来SGD在缓慢的下降，突然乘了一个学习率（0.1），导致其突然下降。在开始阶段，训练的loss比测试的loss高是因为，训练数据做了数据增强，但是测试数据没有做数据增强，噪音很小。

<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-06-27 下午1.49.56.png" alt="截屏2022-06-27 下午1.49.56" style="zoom:33%;" />

<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-06-27 下午4.01.37.png" alt="截屏2022-06-27 下午4.01.37" style="zoom:33%;" />

**Q：什么是Layer responses？**

**A**：做这个的原因是因为，想看一下后面的层数到底有没有被用到。在残差连接中，后面新加上的层，不会使得模型变好的时候，因为有残差连接的存在，就不会学到任何东西，模型就不会动。【但其实第一个模型好像并没有收敛】



## Discussion：

残差网络的优点是：

- 对于深层的网络，残差网络可以使得模型train的起来

- 对于浅层的网络，虽然和普通网络有差不多的精度，但是其收敛速度更快

- 具体的原因是因为
  $$
  \frac{\partial f(g(x))}{\partial x}=\frac{\partial f(g(x))}{\partial g(x)}·\frac{\partial g(x)}{\partial x}
  \\
  \frac{\partial (f(g(x) + g(x))}{\partial x} = \frac{\partial f(g(x))}{\partial g(x)}·\frac{\partial g(x)}{\partial x} + \frac{\partial g(x)}{\partial x}
  $$
  尽管第一项的梯度很小，但是第二项的梯度很大（无损传播），这样就可以使得模型训练的动，训练比较快。