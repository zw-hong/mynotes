æœ¬å‘¨å­¦çš„ä¸œè¥¿ï¼š

- æå®æ¯…è¯¾ç¨‹week4
- çœ‹äº†ä¸€ç‚¹fairseqå·¥å…·åŒ…
- paper reading ğŸ‘‡



## Neural Machine Translation of Rare Words with Subword Units(BPE)

### Abstract:

NMTå¾€å¾€åŸºäºfixed vocabularyï¼Œ ä½†æ˜¯ç¿»è¯‘æ˜¯ä¸€ä¸ªopen-vocabularyé—®é¢˜ã€‚**OOV problem**

ä¹‹å‰çš„å·¥ä½œåŸºäº back off to a dictionary å»è§£å†³out-of-vocabulary é—®é¢˜ã€‚

æœ¬æ–‡è§£å†³çš„æ–¹æ³•æ˜¯ï¼š by encoding rare and unknown words as sequences of **subword units.** 

æ–‡ç« å¯¹ä¸åŒçš„å•è¯åˆ‡åˆ†æ–¹æ³•è¿›è¡Œäº†è®¨è®ºï¼š

- simple character n-gram model 
- a segmentation based on the  **byte pair encoding**



### Conclusionï¼š

ä¸»è¦è´¡çŒ®ï¼šNMT is capable of open-vocabulary translation by representing rare and unseen words as a sequence of subword units

åœ¨baseline NMTä¸­ï¼Œout-of-vocabulary & rare in-vocabulary ç¿»è¯‘çš„è´¨é‡éƒ½æ˜¯ä¸å¥½çš„

æœªæ¥éœ€è¦å»åšçš„ï¼šlearn the optimal vocabulary size for a translation task. We also believe there is further potential in bilingually informed segmentation algorithms to create more alignable subword units, although the segmentation algorithm cannot rely on the target text at runtime.



### Introduction:

èƒŒæ™¯ï¼šä¸€äº›å¾ˆå¤æ‚çš„å¤åˆè¯å¾ˆéš¾å»ç¿»è¯‘ï¼Œæƒ³åˆ°å»æ‹†åˆ†æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚

##### wod-level: 

å‰äººæå‡ºäº†back-off to a dictionary look-upã€‚å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

- ä¸åŒè¯­è¨€ä¹‹é—´æ„é€ çš„å¤åˆè¯ï¼Œå¯èƒ½å¹¶ä¸å­˜åœ¨ä¸€å¯¹ä¸€çš„å…³ç³»
- å¯¹äºä¸€äº›æ²¡æœ‰è§è¿‡çš„å•è¯ï¼Œåè¯ï¼ˆæœ‰äº›ç›´æ¥copyå°±å¯ä»¥è§£å†³ï¼‰ï¼Œbut morphological changes and transliteration is often required, especially if alphabets differï¼ˆå¾ˆéš¾å¤„ç†ï¼‰.

**subword-levelï¼š**

- æ¨¡å‹ç®€å•ä¸”å¥½
- åœ¨1984å¹´æå‡ºçš„BPEç®—æ³•ä¸Šè¿›è¡Œäº†adaptation
- å°†å•è¯åˆ†å‰²ä¸ºsubwordåºåˆ—ï¼Œé’ˆå¯¹ç¨€æœ‰è¯æ±‡ï¼Œä¸è¦å†å»æŸ¥è¯¢è¯è¡¨ã€‚ç¥ç»ç½‘ç»œæ¨¡å‹å¯ä»¥subwordè¡¨ç¤ºä¸­å­¦ä¹ åˆ°ç»„åˆå’Œç›´è¯‘ç­‰èƒ½åŠ›ï¼Œä¹Ÿå¯ä»¥æœ‰æ•ˆçš„äº§ç”Ÿä¸åœ¨è®­ç»ƒæ•°æ®é›†ä¸­çš„è¯æ±‡ã€‚



### model architecture

RNN ç»“æ„



### Subword Translation 

ç¿»è¯‘å•è¯çš„ç±»åˆ«ï¼š

- named entities
- cognates and loanwords
- morphologically complex word 



#### related work

åŸºäºcharacter-based translationä¹Ÿè¢«æå‡ºæ¥è¿‡ï¼Œè™½ç„¶å¯ä»¥ç¿»è¯‘å‡ºæ¥æ‰€æœ‰çš„å•è¯ï¼Œä½†æ˜¯ç²’åº¦å¤ªå°ï¼Œéš¾ä»¥è®­ç»ƒ

**fixed-length continuous word vectors**

**variable-length representation :**

- minimize the vocabulary size
- without back-off model
- a compact representation of the text itselfï¼ˆè¿™æ ·åšçš„åŸå› æ˜¯å› ä¸º an increase in text length reduces efficiency and increase the distance over which nerual models need to pass informationï¼‰

**æœ¬æ–‡å¯¹ä¸éœ€è¦åˆ†å‰²çš„è¯æ±‡å»ºç«‹äº†ä¸€ä¸ªshortlistï¼Œ å¯¹rare wordsä½¿ç”¨äº†subword unit**



#### BPEç®—æ³•ï¼š

ç‰¹æ€§ï¼š

- a data compression technique 
- iteratively replaces the most frequent pair of byes with a signle, unused byte 



æœ¬æ–‡çš„æ–¹æ³•ï¼š

represent each word as a sequence of characters, plus a special end-of-word symbol 'Â·'

BPEæœ‰ä¸¤ç§æ–¹æ³•ï¼š

- independing encoding 

- learning the encoding on the union of the two vocabulary(joint BPE)

    - å¥½å¤„æ˜¯ improves consistency between the source and the target segmentation 

    

```python
import re, collections
def get_stats(vocab):
  	pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
      	symbols = word.split()
        for i in range(len(symbols) - 1):
          	pairs[symbols[i], symbols[i + 1]] += freq
    return pairs
  
def merge_vocab(pair, v_in):
  	v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
      	w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out
  
vocab = {
  	'l o w </w>' : 5,
  	'l o w e r </w>' : 2,
  	'n e w e s t </w>' : 6, 
  	'w i d e s t </w>' : 3
}
num_merges = 10
for i in range(num_merges):
  	pairs = get_state(vocab)
    best = max(pairs, key=pairs.get)
    vocab = merge_vocab(best, vocab)
   	print(best)
```

æœ€ç»ˆç¬¦å·è¡¨å¤§å°=åˆå§‹å¤§å°+åˆå¹¶æ“ä½œæ¬¡æ•°ã€‚æ“ä½œæ¬¡æ•°æ˜¯å”¯ä¸€çš„è¶…å‚æ•°ã€‚



#### ç®—æ³•æµç¨‹

1. ç¡®å®šsubwordè¯è¡¨å¤§å°

2. ç»Ÿè®¡æ¯ä¸€ä¸ª**è¿ç»­å­—èŠ‚å¯¹**çš„å‡ºç°é¢‘ç‡ï¼Œå¹¶ä¿å­˜ä¸ºcode_fileã€‚è¿™ä¸ªæ˜¯gitä¸­learn-bpeå®Œæˆ

    `subword-nmt learn-bpe -s {num_operations} < {train_file} > {codes_file}`

3. å°†å•è¯æ‹†åˆ†ä¸ºå­—ç¬¦åºåˆ—å¹¶åœ¨æœ«å°¾æ·»åŠ åç¼€â€œ </w>â€ï¼Œè€ŒåæŒ‰ç…§code_fileåˆå¹¶æ–°çš„subwordï¼Œé¦–å…ˆåˆå¹¶é¢‘ç‡å‡ºç°æœ€é«˜çš„å­—èŠ‚å¯¹ã€‚ä¾‹å¦‚å•è¯birthdayï¼Œåˆ†å‰²ä¸º['b', 'i', 'r', 't', 'h', 'd', 'a', 'y</w>']ï¼ŒæŸ¥code_fileï¼Œå‘ç°'th'å‡ºç°çš„æœ€å¤šï¼Œé‚£ä¹ˆåˆå¹¶ä¸º['b', 'i', 'r', 'th', 'd', 'a', 'y</w>']ï¼Œæœ€åï¼Œå­—ç¬¦åºåˆ—åˆå¹¶ä¸º['birth', 'day</w>']ã€‚ç„¶åå»é™¤'</w>',å˜ä¸º['birth', 'day']ï¼Œå°†è¿™ä¸¤ä¸ªè¯æ·»åŠ åˆ°è¯è¡¨ã€‚è¿™ä¸ªæ˜¯apply-bpeå®Œæˆã€‚

    `subword-nmt apply-bpe -c {codes_file} < {test_file} > {out_file}`

4. é‡å¤ç¬¬3æ­¥ç›´åˆ°è¾¾åˆ°ç¬¬2æ­¥è®¾å®šçš„subwordè¯è¡¨å¤§å°æˆ–ä¸‹ä¸€ä¸ªæœ€é«˜é¢‘çš„å­—èŠ‚å¯¹å‡ºç°é¢‘ç‡ä¸º1

5. è·å–è¯è¡¨

â€‹	`	subword-nmt get-vocab --train_file {train_file} --vocab_file {vocab_file}`

6. ç¿»è¯‘ç»“æŸä¹‹åï¼Œéœ€è¦åè§£ç ï¼Œå°±æ˜¯å»é™¤ä¸­é—´ç¬¦å·@@ï¼Œç”Ÿæˆæœ€ç»ˆçš„æ–‡æœ¬ï¼Œå°±æ˜¯æŠŠbirth@@ dayè¿˜åŸä¸ºbirthdayã€‚

â€‹	`sed -r 's/(@@ )|(@@ ?$)//g''`







## BLEU: a Method for Automatic Evaluation of Machine Translation 

ç»™å®šæ ‡å‡†è¯‘æ–‡ï¼š**reference**

ç¥ç»ç½‘ç»œç”Ÿæˆå¥å­ï¼š**candidate**

MT evaluayion system requires two ingredients:

- a numerical "translaiton closeness" metric
- a corpus of good quality human reference translations



#### BLEUä¸­ç²¾ç¡®åº¦çš„è®¡ç®—ï¼š

The primary programming task for a BLEU implementor is to compare n-grams of the candidate with
the n-grams of the reference translation and count the number of matches. 

- one first counts the maximum number of times a word occurs in any single reference translation. 
- Next, one clips the total count of each candidate word by its maximum reference count,2adds these clipped counts up, and divides by the total (**unclipped**) number of candidate words.ã€ä¹‹æ‰€ä»¥è¿™ä¹ˆåšçš„åŸå› æ˜¯å› ä¸ºï¼Œæœ‰äº›æœºç¿»ä¼šovergenerate ''resonableâ€œ çš„ç¿»è¯‘è¯­æ–™ã€‘
- $Coung_{clip} = min(Count, Max\_Ref\_Count)$ä¹Ÿå°±æ˜¯è¯´åœ¨ä¸Šä¸€æ­¥çš„countä¸­ï¼Œä¸èƒ½è¶…è¿‡Max_ref_countï¼Œç„¶åç´¯åŠ ï¼Œè®¡ç®—precision



<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/æˆªå±2022-07-17 ä¸‹åˆ9.16.02.png" alt="æˆªå±2022-07-17 ä¸‹åˆ9.16.02" style="zoom:25%;" />



è¿™ç§ç²¾ç¡®åº¦çš„è®¡ç®—æ–¹æ³•ï¼šæ»¡è¶³äº†adequacy & fluency
$$
p_n = \frac{\sum_{C \in\{\text { Candidates }\}} \sum_{n \text {-gram } \in C} \text { Count }_{\text {clip }}(\text { n-gram })}{\sum_{C^{\prime} \in\{\text { Candidates }\}} \sum_{n \text {-gram } \in C^{\prime}} \operatorname{Count}\left(n-\text { gram }^{\prime}\right)} .
$$


However, modified n-gram precision alone fails to enforce the proper translation length,

éšç€nçš„å¢å¤§ï¼Œç²¾ç¡®åº¦ä¼šå¤§å¹…ä¸‹é™ï¼š**the modified n-gram precision decays roughly exponentially with n**

åœ¨è¿™ä¸ªç²¾åº¦çš„è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œä¹Ÿè€ƒè™‘åˆ°äº†å¥å­é•¿åº¦çš„æƒ©ç½š

#### åŠ å…¥BPæƒ©ç½šå› å­ï¼Œé˜²æ­¢ç¿»è¯‘å¥å­è¿‡çŸ­

$$
\mathrm{BP}= \begin{cases}1 & \text { if } c>r \\ e^{(1-r / c)} & \text { if } c \leq r\end{cases}\\
\mathrm{BLEU} = \mathrm{BP}Â·exp(\sum_{n = 1}^{N}w_nlogp_n)
$$

å…¶ä¸­ræ˜¯effective reference corpus length , c æ˜¯length of the candidate translation 



BLEUå€¼ä¹Ÿå’Œreference çš„æ•°é‡æœ‰å…³ï¼Œå½“ä¸€ä¸ªcandidateå¯¹åº”çš„referenceè¶Šå¤šçš„æ—¶å€™ï¼Œå…¶bleuå€¼è¶Šä½ã€‚é‚£ä¹ˆ how many reference translation do we need ?:å…¶å®å¯ä»¥åªç”¨ä¸€ä¸ªreferenceå°±å¯ä»¥ï¼Œä½†æ˜¯è¦ç¡®ä¿è¿™ä¸ªreferenceçš„è¯­æ–™åº“ä¸æ˜¯åŒä¸€ä¸ªç¿»è¯‘å‡ºæ¥çš„ï¼Œä¿è¯ç¿»è¯‘é£æ ¼çš„å¤šæ ·æ€§å³å¯ã€‚





## Moses: Open Source Toolkit for Statistical Machine Translation

#### Abstract:

novel contributions:

- **support for linguistically motivated factors** 
- **confusion network decoding** ï¼šå…è®¸æœ‰æ­§ä¹‰çš„è¾“å…¥ï¼Œæœ‰ç‚¹åƒbeam searchï¼Ÿ Instead of passing along. the one-best output of the recognizer, a network of different word choices may be exmined by the machine translation 
- **efficient data formats formats for translation models and language models**ï¼šå……åˆ†åˆ©ç”¨å†…å­˜

#### Motivation:

morphological, syntactic, or semantic, è¿™äº›ä¿¡æ¯åœ¨é¢„å¤„ç†å’Œåå¤„ç†é˜¶æ®µéƒ½æ˜¯å¾ˆæœ‰ä»·å€¼çš„

#### Toolkit:

- preprocess the data 
- train the language models and the translation models
- tune the models
- evaluation
- external tools:
    - GIZA++ï¼šword alignment
    - SRILMï¼šlanguage modeling 
- utilityï¼šrun repeatable experienments

#### Factored Translation Model:

**Non-factored SMT**

<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/æˆªå±2022-07-18 ä¸‹åˆ3.54.42.png" alt="æˆªå±2022-07-18 ä¸‹åˆ3.54.42" style="zoom:25%;" />

è¿™ç§æ¨¡å‹å¾€å¾€éƒ½æœ‰ä¸€ä¸ªçŸ­è¯­åº“ï¼Œä¸€ä¸€å¯¹åº”ï¼Œ**å¿½ç•¥äº†å¥å­ç»“æ„ï¼Œä¸Šä¸‹æ–‡ç­‰ä¸€äº›é‡è¦çš„ä¿¡æ¯**ï¼Œåªè€ƒè™‘äº†å¥å­çš„è¡¨é¢ç»“æ„ã€‚

**Factored translation** 

æ„æ€å°±æ˜¯è€ƒè™‘åˆ°äº†å¥å­å½“ä¸­å¾ˆå¤šçš„ä¿¡æ¯ï¼Œæ¯”å¦‚surfaceï¼Œ POS tags & lemmaï¼ˆè¯æ ¹ï¼‰ï¼Œç„¶åè¿™äº›ç‰¹å¾å¯ä»¥ç”±Mosesè„šæœ¬å•ç‹¬æå–å‡ºæ¥ï¼Œæ‰€ä»¥æ˜¯user-definedï¼Œç”¨æˆ·å¯ä»¥è‡ªç”±ç»„åˆé€‰æ‹©ä¸€ä¸ªæœ€optimalçš„configuration

<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/æˆªå±2022-07-18 ä¸‹åˆ3.58.27.png" alt="æˆªå±2022-07-18 ä¸‹åˆ3.58.27" style="zoom:25%;" />

#### Confusion Network Decoding

ï¼ˆ07å¹´ï¼‰é‚£ä¼šçš„æœºç¿»ç³»ç»Ÿåªæ˜¯å°†å¥å­ä¿¡æ¯è¾“å…¥ï¼Œç„¶åç¿»è¯‘å³å¯ã€‚ä½†æ˜¯æœ‰åŒ…æ‹¬è¯­éŸ³ç›´æ¥ç¿»è¯‘ï¼Œè¿™äº›è¾“å…¥è¿˜æ²¡æœ‰è¢«è€ƒè™‘åˆ°ã€‚

Moseså·¥å…·åŒ…é‡Œé¢ä¹Ÿæœ‰å¤„ç†è¿™äº›çš„å·¥å…·ï¼Œ**æ··åˆç½‘ç»œè§£ç å…è®¸è¿™ç§å¤šå¯èƒ½æ€§çš„è¾“å…¥ï¼Œæœ€ç»ˆé€‰æ‹©æœ€ä¼˜è¯‘æ–‡ã€‚**

#### Efficient Data Stuctures for Translation Model & Language model 

å¯¹äºphrase transaltion table

- Prefix tree stucture 
- on demand loading 

å¯¹äºLanguage modelï¼š

- the language model is also converted into this binary format, resulting in a minimal loading time during start-up of the decoder ã€fairseqé‡Œé¢çš„æ•°æ®é¢„å¤„ç†é˜¶æ®µçš„äºŒè¿›åˆ¶åŒ–æ˜¯ä¸æ˜¯å› ä¸ºè¿™æ ·çš„åŸå› ï¼Ÿã€‘
- å¯¹å•è¯é¢„æµ‹å’Œback-off probablities çš„æ•°æ®ä¸ä½¿ç”¨ 4byteæˆ–è€…8byteçš„æµ®ç‚¹æ•°ï¼Œé‡‡ç”¨ã€ç´¢å¼•æœºåˆ¶ã€‘åœ¨binsé‡Œé¢å»å­˜å‚¨æ•°æ®ã€‚
