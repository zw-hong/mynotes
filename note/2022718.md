本周学的东西：

- 李宏毅课程week4
- 看了一点fairseq工具包
- paper reading 👇



## Neural Machine Translation of Rare Words with Subword Units(BPE)

### Abstract:

NMT往往基于fixed vocabulary， 但是翻译是一个open-vocabulary问题。**OOV problem**

之前的工作基于 back off to a dictionary 去解决out-of-vocabulary 问题。

本文解决的方法是： by encoding rare and unknown words as sequences of **subword units.** 

文章对不同的单词切分方法进行了讨论：

- simple character n-gram model 
- a segmentation based on the  **byte pair encoding**



### Conclusion：

主要贡献：NMT is capable of open-vocabulary translation by representing rare and unseen words as a sequence of subword units

在baseline NMT中，out-of-vocabulary & rare in-vocabulary 翻译的质量都是不好的

未来需要去做的：learn the optimal vocabulary size for a translation task. We also believe there is further potential in bilingually informed segmentation algorithms to create more alignable subword units, although the segmentation algorithm cannot rely on the target text at runtime.



### Introduction:

背景：一些很复杂的复合词很难去翻译，想到去拆分是一个不错的选择。

##### wod-level: 

前人提出了back-off to a dictionary look-up。存在以下问题：

- 不同语言之间构造的复合词，可能并不存在一对一的关系
- 对于一些没有见过的单词，名词（有些直接copy就可以解决），but morphological changes and transliteration is often required, especially if alphabets differ（很难处理）.

**subword-level：**

- 模型简单且好
- 在1984年提出的BPE算法上进行了adaptation
- 将单词分割为subword序列，针对稀有词汇，不要再去查询词表。神经网络模型可以subword表示中学习到组合和直译等能力，也可以有效的产生不在训练数据集中的词汇。



### model architecture

RNN 结构



### Subword Translation 

翻译单词的类别：

- named entities
- cognates and loanwords
- morphologically complex word 



#### related work

基于character-based translation也被提出来过，虽然可以翻译出来所有的单词，但是粒度太小，难以训练

**fixed-length continuous word vectors**

**variable-length representation :**

- minimize the vocabulary size
- without back-off model
- a compact representation of the text itself（这样做的原因是因为 an increase in text length reduces efficiency and increase the distance over which nerual models need to pass information）

**本文对不需要分割的词汇建立了一个shortlist， 对rare words使用了subword unit**



#### BPE算法：

特性：

- a data compression technique 
- iteratively replaces the most frequent pair of byes with a signle, unused byte 



本文的方法：

represent each word as a sequence of characters, plus a special end-of-word symbol '·'

BPE有两种方法：

- independing encoding 

- learning the encoding on the union of the two vocabulary(joint BPE)

    - 好处是 improves consistency between the source and the target segmentation 

    

```python
import re, collections
def get_stats(vocab):
  	pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
      	symbols = word.split()
        for i in range(len(symbols) - 1):
          	pairs[symbols[i], symbols[i + 1]] += freq
    return pairs
  
def merge_vocab(pair, v_in):
  	v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
      	w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out
  
vocab = {
  	'l o w </w>' : 5,
  	'l o w e r </w>' : 2,
  	'n e w e s t </w>' : 6, 
  	'w i d e s t </w>' : 3
}
num_merges = 10
for i in range(num_merges):
  	pairs = get_state(vocab)
    best = max(pairs, key=pairs.get)
    vocab = merge_vocab(best, vocab)
   	print(best)
```

最终符号表大小=初始大小+合并操作次数。操作次数是唯一的超参数。



#### 算法流程

1. 确定subword词表大小

2. 统计每一个**连续字节对**的出现频率，并保存为code_file。这个是git中learn-bpe完成

    `subword-nmt learn-bpe -s {num_operations} < {train_file} > {codes_file}`

3. 将单词拆分为字符序列并在末尾添加后缀“ </w>”，而后按照code_file合并新的subword，首先合并频率出现最高的字节对。例如单词birthday，分割为['b', 'i', 'r', 't', 'h', 'd', 'a', 'y</w>']，查code_file，发现'th'出现的最多，那么合并为['b', 'i', 'r', 'th', 'd', 'a', 'y</w>']，最后，字符序列合并为['birth', 'day</w>']。然后去除'</w>',变为['birth', 'day']，将这两个词添加到词表。这个是apply-bpe完成。

    `subword-nmt apply-bpe -c {codes_file} < {test_file} > {out_file}`

4. 重复第3步直到达到第2步设定的subword词表大小或下一个最高频的字节对出现频率为1

5. 获取词表

​	`	subword-nmt get-vocab --train_file {train_file} --vocab_file {vocab_file}`

6. 翻译结束之后，需要反解码，就是去除中间符号@@，生成最终的文本，就是把birth@@ day还原为birthday。

​	`sed -r 's/(@@ )|(@@ ?$)//g''`







## BLEU: a Method for Automatic Evaluation of Machine Translation 

给定标准译文：**reference**

神经网络生成句子：**candidate**

MT evaluayion system requires two ingredients:

- a numerical "translaiton closeness" metric
- a corpus of good quality human reference translations



#### BLEU中精确度的计算：

The primary programming task for a BLEU implementor is to compare n-grams of the candidate with
the n-grams of the reference translation and count the number of matches. 

- one first counts the maximum number of times a word occurs in any single reference translation. 
- Next, one clips the total count of each candidate word by its maximum reference count,2adds these clipped counts up, and divides by the total (**unclipped**) number of candidate words.【之所以这么做的原因是因为，有些机翻会overgenerate ''resonable“ 的翻译语料】
- $Coung_{clip} = min(Count, Max\_Ref\_Count)$也就是说在上一步的count中，不能超过Max_ref_count，然后累加，计算precision



<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-07-17 下午9.16.02.png" alt="截屏2022-07-17 下午9.16.02" style="zoom:25%;" />



这种精确度的计算方法：满足了adequacy & fluency
$$
p_n = \frac{\sum_{C \in\{\text { Candidates }\}} \sum_{n \text {-gram } \in C} \text { Count }_{\text {clip }}(\text { n-gram })}{\sum_{C^{\prime} \in\{\text { Candidates }\}} \sum_{n \text {-gram } \in C^{\prime}} \operatorname{Count}\left(n-\text { gram }^{\prime}\right)} .
$$


However, modified n-gram precision alone fails to enforce the proper translation length,

随着n的增大，精确度会大幅下降：**the modified n-gram precision decays roughly exponentially with n**

在这个精度的计算过程中，也考虑到了句子长度的惩罚

#### 加入BP惩罚因子，防止翻译句子过短

$$
\mathrm{BP}= \begin{cases}1 & \text { if } c>r \\ e^{(1-r / c)} & \text { if } c \leq r\end{cases}\\
\mathrm{BLEU} = \mathrm{BP}·exp(\sum_{n = 1}^{N}w_nlogp_n)
$$

其中r是effective reference corpus length , c 是length of the candidate translation 



BLEU值也和reference 的数量有关，当一个candidate对应的reference越多的时候，其bleu值越低。那么 how many reference translation do we need ?:其实可以只用一个reference就可以，但是要确保这个reference的语料库不是同一个翻译出来的，保证翻译风格的多样性即可。





## Moses: Open Source Toolkit for Statistical Machine Translation

#### Abstract:

novel contributions:

- **support for linguistically motivated factors** 
- **confusion network decoding** ：允许有歧义的输入，有点像beam search？ Instead of passing along. the one-best output of the recognizer, a network of different word choices may be exmined by the machine translation 
- **efficient data formats formats for translation models and language models**：充分利用内存

#### Motivation:

morphological, syntactic, or semantic, 这些信息在预处理和后处理阶段都是很有价值的

#### Toolkit:

- preprocess the data 
- train the language models and the translation models
- tune the models
- evaluation
- external tools:
    - GIZA++：word alignment
    - SRILM：language modeling 
- utility：run repeatable experienments

#### Factored Translation Model:

**Non-factored SMT**

<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-07-18 下午3.54.42.png" alt="截屏2022-07-18 下午3.54.42" style="zoom:25%;" />

这种模型往往都有一个短语库，一一对应，**忽略了句子结构，上下文等一些重要的信息**，只考虑了句子的表面结构。

**Factored translation** 

意思就是考虑到了句子当中很多的信息，比如surface， POS tags & lemma（词根），然后这些特征可以由Moses脚本单独提取出来，所以是user-defined，用户可以自由组合选择一个最optimal的configuration

<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-07-18 下午3.58.27.png" alt="截屏2022-07-18 下午3.58.27" style="zoom:25%;" />

#### Confusion Network Decoding

（07年）那会的机翻系统只是将句子信息输入，然后翻译即可。但是有包括语音直接翻译，这些输入还没有被考虑到。

Moses工具包里面也有处理这些的工具，**混合网络解码允许这种多可能性的输入，最终选择最优译文。**

#### Efficient Data Stuctures for Translation Model & Language model 

对于phrase transaltion table

- Prefix tree stucture 
- on demand loading 

对于Language model：

- the language model is also converted into this binary format, resulting in a minimal loading time during start-up of the decoder 【fairseq里面的数据预处理阶段的二进制化是不是因为这样的原因？】
- 对单词预测和back-off probablities 的数据不使用 4byte或者8byte的浮点数，采用【索引机制】在bins里面去存储数据。
