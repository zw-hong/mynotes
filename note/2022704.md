# 2022.6.27

这周做学的东西：

- 李宏毅的深度学习课程week3

- 课程的课后作业

- 看了点前面的东西

  

## paper reading 

<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-07-02 上午9.46.58.png" alt="截屏2022-07-02 上午9.46.58" style="zoom: 33%;" />

BERT是一个双向的结构，可以考虑到句子当中的前后背景信息。只需要加入task-specific输出层就可以得到state-of-the-art模型

==**Q：什么是pre-train?**==

**A：**先用大量的数据预训练一个通用的模型，然后在通过fine-tuning使其适用于下游的任务。Language representation model 不同于language model（针对语言生成的）

==**Q：Language representation model 和 language model**==

**A：**为了区别于针对语言生成的 Language Model，作者给通用的语言模型，取了一个名字，叫语言表征模型 Language Representation Model。深度学习就是表征学习。



### introduction

预训练模型在NLP任务应用主要有：

- sentence-level tasks
- token-level tasks

将预训练的模型应用到下游任务中，主要有feature-based & fine-tuning 两种策略。



feature-based的一个模型是ELMo，使用的是一个RNN的结构，将预训练出来的模型表征作为额外的输入。

==**Q：什么是feature base的方法**==

**A：**1. 首先在大的语料A上【无监督】地训练**语言模型LM**，训练完毕得到语言模型LM

2. 然后构造task-specific model例如序列标注模型，采用【有标记的语料B】来有监督地训练task-sepcific model，**将语言模型的参数固定**，**语料B的训练数据经过语言模型得到LM embedding，作为task-specific model的额外特征**



fine-tunning 的一个模型的是GPT，使用的是transformer 的 decoder 结构，对模型参数进行微调。这种单向的结构是非常局限的，因为在问题回答任务上，是必须要考虑左右上下文信息的。

==**Q：什么是fine-tuning方法？**==

**A：**1.构造语言模型，采用大的语料A来训练语言模型

2. 在语言模型基础上增加少量神经网络层来完成specific task例如序列标注、分类等，然后采用**有标记的语料B来有监督地训练模型**，这个过程中语言模型的参数并不固定，依然是trainable variables.





本文提出了一个BERT模型，主要用到了，提出来就是用来解决上面两个NLP的任务

- masked language model (MLM)：在输入序列中，随机地选择一些token进行mask，然后基于上下文信息对这些masked的单词进行预测。可以融合上下文的信息
- Next sentence predictio （jointly pretrains **text-pair** representations)



本文作出了如下贡献：

- **bidirectional** pre-training for language representation is very important 
- 预训练模型减少了工程量



### BERT

#### Bert的模型结构：

![截屏2022-07-04 下午2.02.28](/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-07-04 下午2.02.28.png)

是基于transformer的深度双向语言表征模型，利用transformer结构创造了一个多层的双向的encoder网络，所有层都可以联合上下文语境进行预训练。不同于GPT和ELMo这两个模型

==**Q：为什么只用到了encoder？**==

**A：**Bert的目标是生成预训练语言模型，所以只需要encoder的机制。transformer的encoder是一次性读取整个文本序列，不是从左到右或者从右到左顺序读取。decoder的部分可以根据不同的task-specific 进行添加。



**BERT的一个优点就是，预训练模型的结构和下游任务的模型结构基本一致**



Bert模型的复杂度，跟层数是一个线性的关系，跟宽度是一个平方的关系（自注意力机制的复杂度）。深度变成了两倍。因为深度增加了两倍，选择1024是使得这个增加的平方大概是2倍，头的维度固定在64（transformer的原理，回头再看看）





#### Bert模型的输入

切词方法使用的是【wordpiece】，思想是如果一个词在整个里面出现概率不大的话，就进行切分看子序列，观察子序列是否有出现的词根。

第一个词【CLS】（classification）

因为是直接进行句子的拼接，有两个方法对句子进行区分：

- 加入【SEP】

- 学习一个嵌入层，来表示第一个句子还是第二个句子

  

![截屏2022-07-02 下午7.06.36](/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-07-02 下午7.06.36.png)

输入其实就是三个embedding层的求和



输入文本在送入token embeddings 层之前需要先进行tokenization 处理，每个token都会被转换成向量。segment embeddding 就是用来区分是第一个句子还是第二个句子【index】。使用position embeddings，是因为在transformer中，不同的位置出现的相同词给出的是同样的输出向量表示。Transformer 中通过植入关于 Token 的相对位置或者绝对位置信息来表示序列的顺序信息。作者测试用学习的方法来得到 Position Embeddings，最终发现固定位置和相对位置效果差不多，所以最后用的是固定位置的，而正弦可以处理更长的 Sequence，且可以用前面位置的值线性表示后面的位置。





#### pre-training Bert



<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-07-04 下午4.48.17.png" alt="截屏2022-07-04 下午4.48.17" style="zoom:25%;" />

==**Q：以往的双向多层网络会使得间接“窥探”到自己，所以引入了mask，这个窥探是怎么产生的？**==

**A：**因为随着网络的加深，就会导致模型间接地窥探到需要预测的词。因为在第一个输出层之后，每一个都带有下面输入层【原本位置上】的信息，那么在下一层就会窥探到，失去意义。【第二个位置上的A，在下一层会传到第一个位置上】

<img src="https://img-blog.csdnimg.cn/2018121121104635.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BpcGlzb3JyeQ==,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 33%;" />

- masked LM（MLM）
  - 缺点是：预训练的数据和fine-tuning的数据不一样，因为mask并没有在fine-tuning阶段出现
  - 解决方法是：80%的token直接替换为mask，10%的数据替换为随机的一个token，10%的数据不做变换【Appendix C.2 通过实验数据得到】

==**Q：为什么要做mask呢？**==

**A：**Unfortunately, standard conditional language models can only be trained left-to-right *or* right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context. 

==**Q：为什么MLM就可以用来训练双向预训练模型？**==

A：我觉得这里的双向其实是因为，mask掉一个东西，就是做cloze task，因为是encoder结构，可以看到所有的信息，这样来预测这个mask就具有了上下文的信息在里面。

==**Q：为什么这样就能解决mismatch的问题呢 80%, 20%, 10%？**==

**A：**The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words, so it is forced to keep a 【distributional contextual representation】 of every input token.

==**Q：加入random token的原因**==

**A：**如果 [MASK] 以外的部分全部都用原 token，模型会学到**『如果当前词是 [MASK]，就根据其他词的信息推断这个词；如果当前词是一个正常的单词，就直接抄输入』**。这样一来，在 finetune 阶段，所有词都是正常单词，模型就照抄所有词，**不提取单词间的依赖关系了。**

**而加入random token，就是提醒模型不要认为是正常的单词，就直接抄输入**，==要根据上下文进行推断==

==**Q：不全使用mask的原因**==

**A：**不全用 [MASK] 是因为在 finetune 到下游任务的时候（例如 POS Tagging）所有词都是已知的，如果模型只在带 [MASK] 的句子上预训练过，那么**模型就只知道根据其他词的信息来预测当前词**，**而不会直接利用这个词本身的信息，会凭空损失一部分信息，对下游任务不利**。





- next sentence prediction （NSP）

==**Q：引入NSP的原因是？**==

**A：**在许多下游任务中，比如问答系统QA和自然语言推理NLI，都是建立在理解两个文本句子之间的关系基础上，并不是语言模型可以捕捉到的。（使得模型可以理解句子之间的关系）【但其实有些paper认为并没有效果。。。】





#### Fine-tuning

<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-07-04 下午4.24.57.png" alt="截屏2022-07-04 下午4.24.57" style="zoom: 25%;" />

- 句子情感分析：取出【CLS】利用softmax即可
- token的分析，比如词性标注等：取出后面的每一个token的最后层作softmax

==**Q：Bert 的微调方式是怎么样的？**==

**A：**For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. 整体都调



#### Experienments

<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-07-04 下午5.12.57.png" alt="截屏2022-07-04 下午5.12.57" style="zoom:25%;" />

warm up ：先增后减（至于增加到什么地方是一个hyperparameter）

- ==为什么需要warm up？==

  ==可能是因为$\sigma$是一个统计的结果，需要大量的数据才能得到一个比较好的结果。所以在前期，将学习率调大，让模型先探索，得到更多的error surface的结果，最后再根据时间的下降，将学习率调低，逐步逼近global minima==

<img src="/Users/zhuwenhong/Library/Application Support/typora-user-images/截屏2022-06-28 上午11.01.05.png" alt="截屏2022-06-28 上午11.01.05" style="zoom:25%;" />

## Discussion

==**Q：双向语言模型指的是transformer的自注意力机制还是输入给transformer的信息包括了上下文？**==

A：应该是包含了上下文，比简单的双向RNN拼接要好